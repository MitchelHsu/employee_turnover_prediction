{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(os.pardir, 'p_season.csv'))\n",
    "target = pd.read_csv(os.path.join(os.pardir, '.csv'))\n",
    "target = target.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57728, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [57728, 14956]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b4eaccc60e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Shuffle data&target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \"\"\"\n\u001b[1;32m    685\u001b[0m     \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    568\u001b[0m                                                     n_samples))\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [57728, 14956]"
     ]
    }
   ],
   "source": [
    "#Shuffle data&target\n",
    "dataset, target = shuffle(dataset, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "kfold = KFold(10, True)\n",
    "predicted = []\n",
    "expected = []\n",
    "\n",
    "for train, test in kfold.split(dataset):\n",
    "    x_train = dataset.iloc[train]\n",
    "    y_train = target.iloc[train]\n",
    "    x_test = dataset.iloc[test]\n",
    "    y_test = target.iloc[test]\n",
    "    svm = OneVsOneClassifier(SVC(gamma='scale')).fit(x_train, y_train)\n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(svm.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.485779619837073\n",
      "Micro-average: 0.9446914952751528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     13596\n",
      "           1       0.00      0.00      0.00       796\n",
      "\n",
      "    accuracy                           0.94     14392\n",
      "   macro avg       0.47      0.50      0.49     14392\n",
      "weighted avg       0.89      0.94      0.92     14392\n",
      "\n",
      "[[13596     0]\n",
      " [  796     0]]\n",
      "Accuracy: 94.47%\n",
      "\n",
      "\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "Average = macro\n",
      "precision: 0.4723457476375764\n",
      "recall: 0.5\n",
      "F1-score: 0.485779619837073\n",
      "\n",
      "\n",
      "Average = micro\n",
      "precision: 0.9446914952751528\n",
      "recall: 0.9446914952751528\n",
      "F1-score: 0.9446914952751528\n",
      "\n",
      "\n",
      "Average = weighted\n",
      "precision: 0.8924420212452041\n",
      "recall: 0.9446914952751528\n",
      "F1-score: 0.9178237508761596\n",
      "\n",
      "\n",
      "Fbeta score: 0.0\n",
      "F1-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Macro-average: {0}'.format(metrics.f1_score(expected, predicted, average='macro')))\n",
    "print('Micro-average: {0}'.format(metrics.f1_score(expected, predicted, average='micro')))\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "accuracy = metrics.accuracy_score(expected, predicted)\n",
    "print('Accuracy: %.2f%%' % (accuracy*100))\n",
    "\n",
    "print('\\n')\n",
    "print('precision:', metrics.precision_score(expected, predicted))\n",
    "print('recall:', metrics.recall_score(expected, predicted))\n",
    "\n",
    "print('Average = macro')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='macro'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='macro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='macro'))\n",
    "\n",
    "print('\\n')\n",
    "print('Average = micro')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='micro'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='micro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='micro'))\n",
    "\n",
    "print('\\n')\n",
    "print('Average = weighted')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='weighted'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='micro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='weighted'))\n",
    "\n",
    "print('\\n')\n",
    "print('Fbeta score:', metrics.fbeta_score(expected, predicted, beta=1.5))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
