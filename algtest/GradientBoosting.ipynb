{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(os.pardir, 'p_train.csv'))\n",
    "target = pd.read_csv(os.path.join(os.pardir, 'PerStatus.csv'))\n",
    "target = target.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle data&target\n",
    "dataset, target = shuffle(dataset, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "kfold = KFold(10, True)\n",
    "predicted = []\n",
    "expected = []\n",
    "\n",
    "for train, test in kfold.split(dataset):\n",
    "    x_train = dataset.iloc[train]\n",
    "    y_train = target.iloc[train]\n",
    "    x_test = dataset.iloc[test]\n",
    "    y_test = target.iloc[test]\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(x_train, y_train)\n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(gb.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.5031661326418861\n",
      "Micro-average: 0.9433713173985547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     13596\n",
      "           1       0.31      0.02      0.04       796\n",
      "\n",
      "    accuracy                           0.94     14392\n",
      "   macro avg       0.63      0.51      0.50     14392\n",
      "weighted avg       0.91      0.94      0.92     14392\n",
      "\n",
      "[[13562    34]\n",
      " [  781    15]]\n",
      "Accuracy: 94.34%\n",
      "Average = macro\n",
      "precision: 0.6258354000458163\n",
      "recall: 0.5081717427975416\n",
      "F1-score: 0.5031661326418861\n",
      "\n",
      "\n",
      "Average = micro\n",
      "precision: 0.9433713173985547\n",
      "recall: 0.9433713173985547\n",
      "F1-score: 0.9433713173985547\n",
      "\n",
      "\n",
      "Average = weighted\n",
      "precision: 0.9101826605827585\n",
      "recall: 0.9433713173985547\n",
      "F1-score: 0.9190978049593999\n"
     ]
    }
   ],
   "source": [
    "print('Macro-average: {0}'.format(metrics.f1_score(expected, predicted, average='macro')))\n",
    "print('Micro-average: {0}'.format(metrics.f1_score(expected, predicted, average='micro')))\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "accuracy = metrics.accuracy_score(expected, predicted)\n",
    "print('Accuracy: %.2f%%' % (accuracy*100))\n",
    "\n",
    "print('Average = macro')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='macro'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='macro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='macro'))\n",
    "\n",
    "print('\\n')\n",
    "print('Average = micro')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='micro'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='micro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='micro'))\n",
    "\n",
    "print('\\n')\n",
    "print('Average = weighted')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='weighted'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='micro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
