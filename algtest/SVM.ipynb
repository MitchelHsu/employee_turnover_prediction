{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(os.pardir, 'p_season.csv'))\n",
    "target = pd.read_csv(os.path.join(os.pardir, 'test_season.csv'))\n",
    "target = target.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['Unnamed: 0', 'periodQ'], axis=1, inplace=True)\n",
    "target = dataset.pop('PerStatus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57728, 7)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle data&target\n",
    "dataset, target = shuffle(dataset, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "kfold = KFold(10, True)\n",
    "predicted = []\n",
    "expected = []\n",
    "\n",
    "for train, test in kfold.split(dataset):\n",
    "    x_train = dataset.iloc[train]\n",
    "    y_train = target.iloc[train]\n",
    "    x_test = dataset.iloc[test]\n",
    "    y_test = target.iloc[test]\n",
    "    svm = OneVsOneClassifier(SVC(gamma='scale')).fit(x_train, y_train)\n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(svm.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.49652886795743933\n",
      "Micro-average: 0.9862111973392462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     56932\n",
      "           1       0.00      0.00      0.00       796\n",
      "\n",
      "    accuracy                           0.99     57728\n",
      "   macro avg       0.49      0.50      0.50     57728\n",
      "weighted avg       0.97      0.99      0.98     57728\n",
      "\n",
      "[[56932     0]\n",
      " [  796     0]]\n",
      "Accuracy: 98.62%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.0\n",
      "recall: 0.0\n",
      "Average = macro\n",
      "precision: 0.49310559866962306\n",
      "recall: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.49652886795743933\n",
      "\n",
      "\n",
      "Average = micro\n",
      "precision: 0.9862111973392461\n",
      "recall: 0.9862111973392461\n",
      "F1-score: 0.9862111973392462\n",
      "\n",
      "\n",
      "Average = weighted\n",
      "precision: 0.9726125257573094\n",
      "recall: 0.9862111973392461\n",
      "F1-score: 0.9793646587636133\n",
      "\n",
      "\n",
      "Fbeta score: 0.0\n",
      "F1-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Macro-average: {0}'.format(metrics.f1_score(expected, predicted, average='macro')))\n",
    "print('Micro-average: {0}'.format(metrics.f1_score(expected, predicted, average='micro')))\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "accuracy = metrics.accuracy_score(expected, predicted)\n",
    "print('Accuracy: %.2f%%' % (accuracy*100))\n",
    "\n",
    "print('\\n')\n",
    "print('precision:', metrics.precision_score(expected, predicted))\n",
    "print('recall:', metrics.recall_score(expected, predicted))\n",
    "\n",
    "print('Average = macro')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='macro'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='macro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='macro'))\n",
    "\n",
    "print('\\n')\n",
    "print('Average = micro')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='micro'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='micro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='micro'))\n",
    "\n",
    "print('\\n')\n",
    "print('Average = weighted')\n",
    "print('precision:', metrics.precision_score(expected, predicted, average='weighted'))\n",
    "print('recall:', metrics.recall_score(expected, predicted, average='micro'))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted, average='weighted'))\n",
    "\n",
    "print('\\n')\n",
    "print('Fbeta score:', metrics.fbeta_score(expected, predicted, beta=1.5))\n",
    "print('F1-score:', metrics.f1_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
